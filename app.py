# -*- coding: utf-8 -*-
"""IAM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n9QBAClOi_qazkdILRZYVfds34jLM3JC
"""

# -*- coding: utf-8 -*-

import os
import sqlite3
import json
import datetime as dt
from typing import List, Tuple

import feedparser
import numpy as np
import pandas as pd
import streamlit as st
from openai import OpenAI
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
from collections import Counter
import re
import plotly.express as px
import plotly.graph_objects as go
import requests
from gnews import GNews

# ========================== CONFIG ==========================

DB_PATH = "news_articles.db"

RSS_FEEDS = [
    "https://feeds.bbci.co.uk/news/business/rss.xml",
    "https://www.cnbc.com/id/10001147/device/rss/rss.html",
    "https://www.marketwatch.com/marketwatch/rss/topstories",
    "https://www.aljazeera.com/xml/rss/all.xml",
    "https://economictimes.indiatimes.com/markets/rssfeeds/1977021501.cms",
    "https://feeds.reuters.com/reuters/worldNews",
    "https://feeds.reuters.com/reuters/businessNews",
    "https://feeds.reuters.com/reuters/energyNews",
]

OPENAI_EMBED_MODEL = "text-embedding-3-small"
FINBERT_MODEL = "ProsusAI/finbert"
MIN_SIMILARITY = 0.40
MAX_ARTICLES_DEFAULT = 200
GDELT_MAX_RECORDS = 250
GNEWS_MAX_RESULTS = 200

# ========================== DATABASE ==========================

def get_connection():
    conn = sqlite3.connect(DB_PATH, check_same_thread=False)
    conn.execute("""
        CREATE TABLE IF NOT EXISTS articles (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            title TEXT,
            summary TEXT,
            published TEXT,
            link TEXT UNIQUE,
            source TEXT,
            content TEXT,
            embedding TEXT
        );
    """)
    return conn

conn = get_connection()

# ========================== OPENAI CLIENT ==========================

@st.cache_resource
def get_openai_client():
    return OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

# ========================== FINBERT SENTIMENT ==========================

@st.cache_resource
def load_finbert():
    tokenizer = AutoTokenizer.from_pretrained(FINBERT_MODEL)
    model = AutoModelForSequenceClassification.from_pretrained(FINBERT_MODEL)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()
    return tokenizer, model, device

def finbert_sentiment(texts: List[str]) -> List[dict]:
    if not texts:
        return []
    tokenizer, model, device = load_finbert()
    enc = tokenizer(
        texts,
        padding=True,
        truncation=True,
        max_length=256,
        return_tensors="pt"
    ).to(device)
    with torch.no_grad():
        outputs = model(**enc)
        probs = torch.softmax(outputs.logits, dim=1).cpu().numpy()
    id2label = {0: "negative", 1: "neutral", 2: "positive"}
    results = []
    for p in probs:
        idx = int(np.argmax(p))
        results.append({"label": id2label[idx], "score": float(p[idx])})
    return results

# ========================== EMBEDDINGS ==========================

def get_embedding(text: str) -> List[float]:
    client = get_openai_client()
    text = text.replace("\n", " ")
    emb = client.embeddings.create(
        model=OPENAI_EMBED_MODEL,
        input=text
    )
    return emb.data[0].embedding

def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:
    if a.size == 0 or b.size == 0:
        return 0.0
    denom = np.linalg.norm(a) * np.linalg.norm(b)
    if denom == 0:
        return 0.0
    return float(np.dot(a, b) / denom)

# ========================== RSS INGESTION ==========================

def parse_rss(url: str) -> List[dict]:
    feed = feedparser.parse(url)
    articles = []
    for entry in feed.entries:
        title = entry.get("title", "")
        summary = entry.get("summary", "")
        link = entry.get("link", "")
        if hasattr(entry, "published_parsed") and entry.published_parsed:
            pub = dt.datetime(*entry.published_parsed[:6])
        else:
            pub = dt.datetime.utcnow()
        published = pub.isoformat()
        if "content" in entry and entry.content:
            content = entry.content[0].value
        else:
            content = summary
        articles.append({
            "title": title,
            "summary": summary,
            "published": published,
            "link": link,
            "content": content,
            "source": url
        })
    return articles

def fetch_rss_articles() -> int:
    cur = conn.cursor()
    new_count = 0
    for url in RSS_FEEDS:
        try:
            arts = parse_rss(url)
        except Exception:
            continue
        for art in arts:
            try:
                cur.execute("""
                    INSERT OR IGNORE INTO articles
                    (title, summary, published, link, source, content, embedding)
                    VALUES (?, ?, ?, ?, ?, ?, NULL)
                """, (
                    art["title"], art["summary"], art["published"],
                    art["link"], art["source"], art["content"]
                ))
                if cur.rowcount > 0:
                    new_count += 1
            except Exception:
                pass
    conn.commit()
    return new_count

# ========================== GDELT ==========================

def fetch_gdelt_articles(query: str, start: dt.date, end: dt.date) -> int:
    cur = conn.cursor()
    new_count = 0

    start_str = start.strftime("%Y%m%d000000")
    end_str = end.strftime("%Y%m%d235959")

    base_url = (
        "https://api.gdeltproject.org/api/v2/doc/doc"
        f"?query={requests.utils.quote(query)}"
        f"&mode=artlist&maxrecords={GDELT_MAX_RECORDS}&format=json"
        f"&startdatetime={start_str}&enddatetime={end_str}"
    )

    try:
        r = requests.get(base_url, timeout=10)
        if r.status_code != 200:
            return 0
        articles = r.json().get("articles", [])
    except Exception:
        return 0

    for a in articles:
        title = a.get("title", "")
        summary = a.get("seendate", "")
        link = a.get("url", "")
        src = a.get("domain", "gdelt")
        seen = a.get("seendate", "")
        try:
            published = dt.datetime.strptime(seen, "%Y%m%d%H%M%S").isoformat()
        except Exception:
            published = dt.datetime.utcnow().isoformat()
        content = a.get("snippet", "") or summary

        try:
            cur.execute("""
                INSERT OR IGNORE INTO articles
                (title, summary, published, link, source, content, embedding)
                VALUES (?, ?, ?, ?, ?, ?, NULL)
            """, (title, summary, published, link, src, content))
            if cur.rowcount > 0:
                new_count += 1
        except:
            pass
    conn.commit()
    return new_count

# ========================== GOOGLE NEWS ==========================

def fetch_gnews_articles(query: str, start: dt.date, end: dt.date) -> int:
    cur = conn.cursor()
    new_count = 0

    try:
        gn = GNews(language='en', max_results=GNEWS_MAX_RESULTS)
        gn.start_date = (start.year, start.month, start.day)
        gn.end_date = (end.year, end.month, end.day)
        results = gn.get_news(query)
    except Exception:
        return 0

    for r in results:
        title = r.get("title", "")
        link = r.get("url") or r.get("link", "")
        pub = r.get("published date") or r.get("published_date")
        desc = r.get("description", "")
        source = r.get("publisher", {}).get("title", "google-news")
        published = str(pub) if pub else dt.datetime.utcnow().isoformat()

        try:
            cur.execute("""
                INSERT OR IGNORE INTO articles
                (title, summary, published, link, source, content, embedding)
                VALUES (?, ?, ?, ?, ?, ?, NULL)
            """, (title, desc, published, link, source, desc))
            if cur.rowcount > 0:
                new_count += 1
        except:
            pass
    conn.commit()
    return new_count

# ========================== SEMANTIC SEARCH ==========================

def ensure_embeddings(df_ids_emb):
    cur = conn.cursor()
    for _, row in df_ids_emb.iterrows():
        if row["embedding"] is None:
            text = (row["title"] or "") + " " + (row["summary"] or "")
            if not text.strip():
                continue
            emb = get_embedding(text)
            cur.execute(
                "UPDATE articles SET embedding = ? WHERE id = ?",
                (json.dumps(emb), int(row["id"]))
            )
    conn.commit()

def load_articles_for_range(start: dt.date, end: dt.date) -> pd.DataFrame:
    cur = conn.cursor()
    cur.execute("""
        SELECT id, title, summary, published, link, source, content, embedding
        FROM articles
        WHERE date(published) BETWEEN ? AND ?
    """, (start.isoformat(), end.isoformat()))
    rows = cur.fetchall()
    if not rows:
        return pd.DataFrame()
    return pd.DataFrame(rows, columns=[
        "id", "title", "summary", "published", "link",
        "source", "content", "embedding"
    ])

def hybrid_search(query: str, start: dt.date, end: dt.date, top_k: int) -> pd.DataFrame:
    df = load_articles_for_range(start, end)
    if df.empty:
        return df

    df["embedding"] = df["embedding"].apply(
        lambda x: None if x in (None, "", "NULL") else x
    )

    ensure_embeddings(df[["id", "title", "summary", "embedding"]])

    df = load_articles_for_range(start, end)
    if df.empty:
        return df

    q_emb = np.array(get_embedding(query))
    sims = []
    for _, row in df.iterrows():
        emb_vec = np.array(json.loads(row["embedding"]))
        sims.append(cosine_similarity(q_emb, emb_vec))
    df["similarity"] = sims

    q_lower = query.lower().strip()
    kw_mask = (
        df["title"].str.lower().str.contains(q_lower, na=False) |
        df["summary"].str.lower().str.contains(q_lower, na=False) |
        df["content"].str.lower().str.contains(q_lower, na=False)
    )

    sem_mask = df["similarity"] >= MIN_SIMILARITY

    if kw_mask.any() and sem_mask.any():
        mask = kw_mask & sem_mask
        df["match_type"] = np.where(mask, "keyword+semantic",
                                    np.where(kw_mask, "keyword", "semantic"))
        filtered = df[kw_mask | sem_mask]
    elif kw_mask.any():
        df["match_type"] = "keyword"
        filtered = df[kw_mask]
    elif sem_mask.any():
        df["match_type"] = "semantic"
        filtered = df[sem_mask]
    else:
        return pd.DataFrame()

    return filtered.sort_values("similarity", ascending=False).head(top_k)

# ========================== SENTIMENT INDEX ==========================

def extract_top_keywords(titles, n=20):
    text = " ".join(titles).lower()
    words = re.findall(r"[a-zA-Z]{4,}", text)
    stopwords = {
        "this", "that", "with", "from", "will", "have", "been", "into",
        "after", "over", "under", "they", "them", "your", "their", "about",
        "which", "there", "where", "when", "than", "because", "while",
        "before", "after", "through", "within", "without"
    }
    words = [w for w in words if w not in stopwords]
    counter = Counter(words)
    return counter.most_common(n)

def calculate_sentiment_index(df):
    pos = len(df[df["sentiment_label"] == "positive"])
    neg = len(df[df["sentiment_label"] == "negative"])
    total = len(df)
    if total == 0:
        return 0.0
    return round(((pos - neg) / total) * 100, 2)

# ========================== STREAMLIT UI ==========================

def run_app():
    st.set_page_config(page_title="Financial News Sentiment Dashboard", layout="wide")

    st.title("üìà Financial News Sentiment Dashboard")
    st.markdown("Hybrid Search ‚Üí RSS + Google News + GDELT ‚Üí FinBERT Sentiment")

    with st.sidebar:
        st.header("Settings")

        if st.button("Fetch Base RSS"):
            added = fetch_rss_articles()
            st.success(f"Added {added} RSS articles")

        today = dt.date.today()
        start_date = st.date_input("Start Date", today - dt.timedelta(days=14))
        end_date = st.date_input("End Date", today)

        max_articles = st.slider(
            "Max Articles", 50, 500, MAX_ARTICLES_DEFAULT, step=50
        )

        use_gdelt = st.checkbox("Include GDELT", True)
        use_gnews = st.checkbox("Include Google News", True)

    st.subheader("üîç Topic Search")
    query = st.text_input("Enter search topic")
    if not st.button("Analyze"):
        return

    if not query.strip():
        st.warning("Please enter a topic")
        return

    # EXTRA SOURCES
    if use_gdelt or use_gnews:
        with st.spinner("Fetching extra sources‚Ä¶"):
            added = 0
            if use_gdelt:
                added += fetch_gdelt_articles(query, start_date, end_date)
            if use_gnews:
                added += fetch_gnews_articles(query, start_date, end_date)
        st.success(f"Added {added} extra articles")

    # SEARCH
    with st.spinner("Running hybrid search‚Ä¶"):
        df = hybrid_search(query, start_date, end_date, max_articles)

    if df.empty:
        st.error("No matching articles found.")
        return

    # SENTIMENT
    with st.spinner("Running FinBERT sentiment‚Ä¶"):
        texts = df["content"].fillna(df["summary"]).tolist()
        sents = finbert_sentiment(texts)

    df["sentiment_label"] = [s["label"] for s in sents]
    df["sentiment_score"] = [s["score"] for s in sents]
    df["relevance"] = (df["similarity"] * 100).round(1)
    df["source_domain"] = df["source"].fillna("unknown")

    sentiment_index = calculate_sentiment_index(df)

    tab_dash, tab_articles, tab_kw, tab_dl = st.tabs(
        ["üìä Dashboard", "üì∞ Articles", "üîë Keywords", "üì• Download"]
    )

    # DASHBOARD
    with tab_dash:
        st.subheader("Sentiment Overview")
        counts = df["sentiment_label"].value_counts()
        total = int(counts.sum())
        pos = int(counts.get("positive", 0))
        neg = int(counts.get("negative", 0))
        neu = int(counts.get("neutral", 0))

        c1, c2, c3, c4, c5 = st.columns(5)
        c1.metric("Total", total)
        c2.metric("Positive", pos)
        c3.metric("Negative", neg)
        c4.metric("Neutral", neu)
        c5.metric("Sentiment Index", sentiment_index)

        fig = px.pie(
            values=[pos, neg, neu],
            names=["Positive", "Negative", "Neutral"],
            hole=0.3
        )
        st.plotly_chart(fig)

    # ARTICLES
    with tab_articles:
        st.subheader("Articles")
        for _, r in df.iterrows():
            st.markdown(f"### {r['title']}")
            st.caption(r["source_domain"])
            st.caption(f"Relevance: {r['relevance']}% | Sentiment: {r['sentiment_label']}")
            st.markdown(f"[Read Article]({r['link']})")
            st.write(r["summary"])
            st.markdown("---")

    # KEYWORDS
    with tab_kw:
        st.subheader("Top Keywords")
        kw = extract_top_keywords(df["title"].tolist(), n=20)
        if kw:
            kw_df = pd.DataFrame(kw, columns=["Keyword", "Frequency"])
            st.bar_chart(kw_df.set_index("Keyword"))
        else:
            st.info("Not enough text for keywords")

    # DOWNLOAD
    with tab_dl:
        csv = df.to_csv(index=False).encode("utf-8")
        st.download_button("Download CSV", csv, "results.csv", "text/csv")
        st.dataframe(df)

if __name__ == "__main__":
    run_app()
